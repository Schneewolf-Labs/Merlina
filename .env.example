# Merlina Configuration Template
# Copy this file to .env and customize for your environment

# ==========================================
# Server Configuration
# ==========================================

# Server host (0.0.0.0 allows external connections, 127.0.0.1 is localhost only)
HOST=0.0.0.0

# Server port
PORT=8000

# Production domain (leave empty for development)
# Examples:
#   http://localhost:8000
#   https://merlina.example.com
#   http://192.168.1.100:8000
# DOMAIN=

# ==========================================
# Directory Paths (Optional)
# ==========================================

# Uncomment to customize data directories
# DATA_DIR=./data
# MODELS_DIR=./models
# RESULTS_DIR=./results
# UPLOADS_DIR=./data/uploads
# FRONTEND_DIR=./frontend

# Database location
# DATABASE_PATH=./data/jobs.db

# ==========================================
# External Services
# ==========================================

# Weights & Biases API Key
# Get your key from: https://wandb.ai/authorize
# WANDB_API_KEY=

# HuggingFace Token (required for gated models like Llama)
# Get your token from: https://huggingface.co/settings/tokens
# HF_TOKEN=

# ==========================================
# Training Defaults (Optional)
# ==========================================

# Default LoRA settings
# DEFAULT_LORA_R=64
# DEFAULT_LORA_ALPHA=32

# Default training hyperparameters
# DEFAULT_LEARNING_RATE=0.000005
# DEFAULT_BATCH_SIZE=1
# DEFAULT_GRADIENT_ACCUMULATION_STEPS=16
# DEFAULT_NUM_EPOCHS=2

# ==========================================
# Job Queue Configuration
# ==========================================

# Maximum number of training jobs to run simultaneously
# Set to 1 for safety (prevents OOM on single GPU)
# Increase if you have multiple GPUs with sufficient VRAM
# MAX_CONCURRENT_JOBS=1

# ==========================================
# System Configuration
# ==========================================

# GPU selection (comma-separated GPU IDs)
# Examples:
#   CUDA_VISIBLE_DEVICES=0        # Use only GPU 0
#   CUDA_VISIBLE_DEVICES=0,1      # Use GPUs 0 and 1
#   CUDA_VISIBLE_DEVICES=1,2,3    # Use GPUs 1, 2, and 3
# CUDA_VISIBLE_DEVICES=

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ==========================================
# Security & Limits
# ==========================================

# CORS origins (comma-separated list or "*" for all)
# CORS_ORIGINS=*

# Maximum upload file size in MB
# MAX_UPLOAD_SIZE_MB=500
